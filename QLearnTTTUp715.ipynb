{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be86182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #numpy for board positions\n",
    "import pickle #to train the machine and save policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef09bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3 #for set up for the playing t-t-t grid\n",
    "columns = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34798f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State: #state of the game grid, with both the player's and the machine's positions\n",
    "    def __init__(self, Player1, Player2): #initialize the game grid\n",
    "        self.board = np.zeros((rows, columns))\n",
    "        self.Player1 = Player1\n",
    "        self.Player2 = Player2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # Player1 is the machine that plays first\n",
    "        self.playerSymbol = 1 #controls whose turn it is. Updates throughout the game with '1' when P1 makes a move and '2' when P2 makes a move\n",
    "    \n",
    "    # get unique hash of current board state\n",
    "    def getHash(self): #stores the current board \n",
    "        self.boardHash = str(self.board.reshape(columns*rows)) #updates the playing grid with the player moves\n",
    "        return self.boardHash\n",
    "    \n",
    "    def emptySpots(self): #after each turn check which grid spots are empty\n",
    "        positions = []\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j)) #Appends any empty spots to the \"positions\" list in order to let the machine know where it can make next moves\n",
    "        return positions\n",
    "    \n",
    "    def updateState(self, position):  #updates the game board with chosen player moves. Regulates turns.\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1 \n",
    "        \n",
    "    def winner(self): #Checks if the game ha ended. Assigns the winner.\n",
    "        # rows\n",
    "        for i in range(rows):\n",
    "            if sum(self.board[i, :]) == 3: #if any row adds up to 3, then player 1 has won. Game end.\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3: #if any row adds up to -3, then player 2 has won. Game end.\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "            \n",
    "        # columns\n",
    "        for i in range(columns):\n",
    "            if sum(self.board[:, i]) == 3:#if any col adds up to 3, then player 1 has won. Game end.\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:#if any col adds up to -3, then player 1 has won. Game end.\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "            \n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(columns)]) #checks column sum [(0,0), (1,1),(2,2)]\n",
    "        diag_sum2 = sum([self.board[i, columns-i-1] for i in range(columns)]) #(0,2)(1,1)(2,0)\n",
    "        diag_sum = max(diag_sum1, diag_sum2)\n",
    "        \n",
    "        if diag_sum == 3:  #Player 1 wins\n",
    "            self.isEnd = True\n",
    "            return 1\n",
    "        if diag_sum == -3: #Player 2 wins\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.emptySpots()) == 0: #no more empty spots on the grid = no possible winning combos\n",
    "            self.isEnd = True\n",
    "            return 0 #returns neither 1 nor -1, thus no one wins\n",
    "        # not end\n",
    "        self.isEnd = False #Game end.\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1: #Machine wins\n",
    "            self.Player1.feedReward(1)\n",
    "            self.Player2.feedReward(0)\n",
    "        elif result == -1: #Human Player wins\n",
    "            self.Player1.feedReward(0)\n",
    "            self.Player2.feedReward(1)\n",
    "        else: #tie\n",
    "            self.Player1.feedReward(0.1)\n",
    "            self.Player2.feedReward(0.5)\n",
    "    \n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((rows, columns))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    #training the machine by letting it play against itself\n",
    "    def play(self, rounds=100):\n",
    "        for i in range(rounds):\n",
    "            if i%1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.emptySpots() #look for available positions\n",
    "                Player1_action = self.Player1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                #choose the best action\n",
    "                # take action and upate board state\n",
    "                self.updateState(Player1_action) #update the board\n",
    "                board_hash = self.getHash()\n",
    "                self.Player1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # ended with Player1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.Player1.reset()\n",
    "                    self.Player2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.emptySpots()\n",
    "                    Player2_action = self.Player2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(Player2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.Player2.addState(board_hash)\n",
    "                    \n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # ended with Player2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.Player1.reset()\n",
    "                        self.Player2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "    \n",
    "    # play with human\n",
    "    #mostly the same as in machine play but ensures that the machine goes first\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.emptySpots()\n",
    "            Player1_action = self.Player1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(Player1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.Player1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.emptySpots()\n",
    "                Player2_action = self.Player2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(Player2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.Player2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "    def showBoard(self):\n",
    "        # Player1: x  Player2: o\n",
    "        for i in range(0, rows):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, columns):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c26e32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachinePlayer: # the AI opponent\n",
    "    def __init__(self, name, exp_rate=0.3): \n",
    "        #exploration vs exploitation rate --> 70% machine will take greedy action based on estimation of reward,\n",
    "        #other 30% will be random\n",
    "        \n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken in this list\n",
    "        self.lr = 0.2 #learning rate (0-1). Controls how quickly the model is adapted to the problem.\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9 #decay rate between 0 and 1. How much machine cares about future reward\n",
    "        #1 = machine equally cares about all future rewards\n",
    "        #0 = agent cares about only the reward of the current state\n",
    "        self.states_value = {}  # state -> value; updates corresponding to states\n",
    "    \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(columns*rows))\n",
    "        return boardHash\n",
    "    \n",
    "    #store the hash of board state into state-value dictnary\n",
    "    #exploitation\n",
    "    #hash the next state and choose the action with next max reward \n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate: #choose a random square on the playing grid to start the game\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    # \"the updated value of state t equals the current value of state t adding the difference between the value \n",
    "    #of next state and the value of current state, which is multiplied by a learning rate α (Given the reward of \n",
    "    #intermediate state is 0)\"\n",
    "    \n",
    "    #positions stored in self.states\n",
    "    #at the end of the game, estimates are updated in 'reversed' fashion\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr*(self.decay_gamma*reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    " \n",
    " #save the policy the machine learned during training in order to later apply it to match against human\n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()\n",
    "\n",
    "class HumanPlayer: #human class to play against the machine\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def chooseAction(self, positions): #ask human to input their actions\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions: #ask again if they choose an already occupied slot\n",
    "                return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03fa190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Rounds 0\n",
      "Rounds 1000\n",
      "Rounds 2000\n",
      "Rounds 3000\n",
      "Rounds 4000\n",
      "Rounds 5000\n",
      "Rounds 6000\n",
      "Rounds 7000\n",
      "Rounds 8000\n",
      "Rounds 9000\n",
      "Rounds 10000\n",
      "Rounds 11000\n",
      "Rounds 12000\n",
      "Rounds 13000\n",
      "Rounds 14000\n",
      "Rounds 15000\n",
      "Rounds 16000\n",
      "Rounds 17000\n",
      "Rounds 18000\n",
      "Rounds 19000\n",
      "Rounds 20000\n",
      "Rounds 21000\n",
      "Rounds 22000\n",
      "Rounds 23000\n",
      "Rounds 24000\n",
      "Rounds 25000\n",
      "Rounds 26000\n",
      "Rounds 27000\n",
      "Rounds 28000\n",
      "Rounds 29000\n",
      "Rounds 30000\n",
      "Rounds 31000\n",
      "Rounds 32000\n",
      "Rounds 33000\n",
      "Rounds 34000\n",
      "Rounds 35000\n",
      "Rounds 36000\n",
      "Rounds 37000\n",
      "Rounds 38000\n",
      "Rounds 39000\n",
      "Rounds 40000\n",
      "Rounds 41000\n",
      "Rounds 42000\n",
      "Rounds 43000\n",
      "Rounds 44000\n",
      "Rounds 45000\n",
      "Rounds 46000\n",
      "Rounds 47000\n",
      "Rounds 48000\n",
      "Rounds 49000\n"
     ]
    }
   ],
   "source": [
    "p1 = MachinePlayer(\"p1\")  #train the machine against itself\n",
    "p2 = MachinePlayer(\"p2\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"Training\")\n",
    "st.play(50000) #play against itself 50,000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e103191",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.savePolicy() #save the training\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1280e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.loadPolicy(\"policy_p1\") #use training in game against human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d3c7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "Input your action row:1\n",
      "Input your action col:1\n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "Input your action row:2\n",
      "Input your action col:0\n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "Input your action row:0\n",
      "Input your action col:1\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| o |   |   | \n",
      "-------------\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "| o | x |   | \n",
      "-------------\n",
      "Input your action row:1\n",
      "Input your action col:1\n",
      "Input your action row:1\n",
      "Input your action col:2\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n",
      "| o | x |   | \n",
      "-------------\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "tie!\n"
     ]
    }
   ],
   "source": [
    "p1 = MachinePlayer(\"computer\", exp_rate=0)\n",
    "p1.loadPolicy(\"policy_p1\")\n",
    "\n",
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.play2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62c78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MarTensor] *",
   "language": "python",
   "name": "conda-env-MarTensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
